# -*- coding: utf-8 -*-
"""crispr_2023_crispr_Exercise1_20230731

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZJ_a8LY0WJweSDzcYCtXIjOhYc-W3qMC

# Warming up excercise
Below you will find the first excercise. In which you will be introduced to the basic concepts of Tensorflow / Keras and these concepts will be used to train a small CRISPR ontarget efficiency model on real data.

## basic code definitions
Eter the cell below and press play or Ctrl+Enter in the block below to execute. You should see the message "Definitions executed" printed after execution. (<5 minutes)
"""

#!/usr/bin/env python3
from google.colab import drive
from random import randint
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
import pickle
import urllib3
import subprocess
import os

from tensorflow.keras import models, callbacks, Model, Input, utils, metrics
from tensorflow.keras.layers import Conv1D, Dropout, AveragePooling1D, Flatten, Dense, concatenate, SpatialDropout1D

eLENGTH30 = 30
eDEPTH = 4

# Function to convert DNA sequence to one-hot encoding
def onehot(x):
    z = list()
    for y in list(x):
        if y in "Aa":
            z.append(0)
        elif y in "Cc":
            z.append(1)
        elif y in "Gg":
            z.append(2)
        elif y in "TtUu":
            z.append(3)
        else:
            print("Non-ATGCU character in", x)
            raise Exception
    return z

# Function to set the data into the appropriate format
def set_data(DX, s):
    if s is None:
        return
    for j, x in enumerate(onehot(s)):
        DX[j][x] = 1


# Preprocessing function for the sequence data
def preprocess_seq(data):
    DATA_X30 = np.zeros((len(data), eLENGTH30, eDEPTH), dtype=np.float32)  # onehot
    DATA_G = np.zeros((len(data), 1), dtype=np.float32)  # deltaGb
    DATA_Y = np.zeros((len(data)), dtype=np.float32)  # efficiency

    for l, d in enumerate(data):
        set_data(DATA_X30[l], d[0])
        DATA_G[l] = -d[1]
        DATA_Y[l] = d[2]
    return (DATA_X30, DATA_G, DATA_Y)

print('Definitions executed')

"""## Excercise 1.1 (<5minutes)
The sequences of the guides/ontargets, e.g. ACTGAAAAACCCCCTTTTTGGG, needs to be onehot encoded. Get the onehot encoding of the suggested ontarget (ACTGAAAAACCCCCTTTTTGGG)  using the onehot function defined above. Is this what a onehot encoding is supposed to look like, and what could be wrong with encoding the sequence this way?
"""

#solution
onehot('ACTGAAAAACCCCCTTTTTGGG')

"""## Excercise 1.2
Upload the provided training and validation data for CRISPRon using the  📁 button in the left margin of the page. Then excecute the code below to load the data into the notebook.
"""

# x30 - onehot encoded 30mer
# g - deltaGb
# y - the efficiency value [0-100] )

# Training Data
PATH = './'
with open(PATH+'/training_data.pickle', 'rb') as f:
    d = pickle.load(f)
(x30, g, y) = preprocess_seq(d)

#Validation data read
with open(PATH+'/validation_data.pickle', 'rb') as f:
    dv = pickle.load(f)
(x30v, gv, yv) = preprocess_seq(dv)

"""### Excercise 1.2.1  (<5 minutes)
Get the first value of the raw unprocessed training data (d[0]) and the first values of the processed data (x30, g, y). Is this what you expected for the onehot encoding?
"""

#solution
print(d[0])
print(x30[0], g[0], y[0], sep='\n')

"""### Excercise 1.2.2 Model definition (5-10 minutes)
In the code below, a simplified version of the CRISPRon ontarget model is defined. Review the code without diving into the details. Then execute it to load the model.
"""

OPT = 'adam' #use the ADAM optizer
LOSS = 'mse' #loss function is mean squared error

DROPOUT_CNV = 0.1
DROPOUT_DENSE = 0.3


CONV_1_SIZE = 3
N_CONV_1 = 20
N_DENSE = 40
N_OUT = 40

# Inputs
inputs_30 = list()

inputs_c_30 = Input(shape=(eLENGTH30, eDEPTH), name="inputs_onehot_30")
inputs_30.append(inputs_c_30)
inputs_g = Input(shape=(1), name="inputs_dgb_on")
inputs_30.append(inputs_g)

# Model_30 layers
for_dense_30 = list()

# First convolution layer
conv1_out_30 = Conv1D(N_CONV_1, CONV_1_SIZE, activation='relu', input_shape=(eLENGTH30, eDEPTH), name="conv_3_30")(inputs_c_30)
conv1_dropout_out_30 = SpatialDropout1D(DROPOUT_CNV, name="drop_1_30")(conv1_out_30)
conv1_pool_out_30 = AveragePooling1D(2, padding='SAME', name="pool_1_30")(conv1_dropout_out_30)
conv1_flatten_out_30 = Flatten(name="flatten_3_30")(conv1_pool_out_30)
for_dense_30.append(conv1_flatten_out_30)

# Concatenation of conv layers and deltaGb layer
concat_out_30 = concatenate(for_dense_30, name="concat_cnv_30")

# First dense (fully connected) layer
dense0_out_30 = Dense(N_DENSE, activation='relu', name="dense_0_30")(concat_out_30)
dense0_dropout_out_30 = Dropout(DROPOUT_DENSE, name="drop_d0_30")(dense0_out_30)

# Gb input used raw
concat1_out_30 = concatenate((inputs_g, dense0_dropout_out_30), name="concat_dgb_30")

# First dense (fully connected) layer
dense1_out_30 = Dense(N_DENSE, activation='relu', name="dense_1_30")(concat1_out_30)
dense1_dropout_out_30 = Dropout(DROPOUT_DENSE, name="drop_d1_30")(dense1_out_30)

# Second dense (fully connected) layer
dense2_out_30 = Dense(N_OUT, activation='relu', name="dense_2_30")(dense1_dropout_out_30)
dense2_dropout_out_30 = Dropout(DROPOUT_DENSE, name="drop_d2_30")(dense2_out_30)

# Output layer
dense_on_off_out = Dense(N_OUT, activation='relu', name="dense_on_off")(dense2_dropout_out_30)
dense_on_off_dropout_out = Dropout(DROPOUT_DENSE, name="drop_d_on_off")(dense_on_off_out)
output_30 = Dense(1, name="output_30")(dense_on_off_dropout_out)

# Model_30
postfix = '.model_30'
inputs = inputs_30
outputs = [output_30]
model = Model(inputs=inputs, outputs=outputs)
model.compile(loss=LOSS, optimizer=OPT, metrics=['mae', 'mse'])

"""### Excercise 1.2.3   (<10 minutes)
Use model.summary and keras.utils.plot to review the model details. Where are inputs and outputs. Identify the convolutional and multi-layer perceptron parts of the model. Wheres is the ΔGb input inserted into the model?
"""

#solution
model.summary()
keras.utils.plot_model(model)

"""### Exercise 1.2.4 Model training (10 minutes)
Below you will find code for training the simplified CRISPRon model on the provided training data, using the validation data for model evaluation during training. Familiarize yourself with the code and parameters. What is the difference between BATCH_SIZE and epochs?

Execute the model **training**
"""

print("training...")

OPT = 'adam' #use the ADAM optizer
LEARN = 1e-4 #learning rate
EPOCHS = 200 #maximum number of Epochs
LOSS = 'mse' #loss function is mean squared error
BATCH_SIZE = 64 #batch size for the traing

#save the initial weights, or restore them if they exists
if os.path.exists('model_30.init.h5'):
  model.load_weights('model_30.init.h5')
else:
  model.save_weights('model_30.init.h5')


# Training
history = model.fit(
    (x30, g), y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=((x30v,gv), yv),
    callbacks=[
        #early stopping is a way to control for overfitting and save the best model
        callbacks.EarlyStopping(
            monitor='val_loss',
            min_delta=0.1,
            patience=25,
            verbose=1,
            mode='auto',
            restore_best_weights=True,
            ),

    ],
    verbose=1
)



print("done")

"""### Exercise 1.2.4.1
How many epochs did the code use to converge. When did it reach the optimimal model. Compare the mean squared error and mean absolute error you obtain for the best model with those obtained for the full model trained on the same date (mae XX, mse XX)

"""

model.evaluate((x30v,gv), yv)

"""### Exercise 1.2.5 (if time allows)
Play with the model and parameters to get a better performance. What is needed to get near the full the model performance?
"""